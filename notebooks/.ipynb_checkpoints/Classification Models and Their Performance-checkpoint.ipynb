{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/ADNIMERGE_train.csv\")\n",
    "df_test = pd.read_csv(\"data/ADNIMERGE_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['RID', 'DX_bl'], axis=1).copy()\n",
    "y_train = df_train['DX_bl'].copy()\n",
    "X_test = df_test.drop(['RID', 'DX_bl'], axis=1).copy()\n",
    "y_test = df_test['DX_bl'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to help compare the accuracy of models\n",
    "def score(model, X_train, y_train, X_test, y_test):\n",
    "    train_acc = model.score(X_train,y_train)\n",
    "    test_acc = model.score(X_test,y_test)\n",
    "    test_class0 = model.score(X_test[y_test==0], y_test[y_test==0])\n",
    "    test_class1 = model.score(X_test[y_test==1], y_test[y_test==1])\n",
    "    test_class2 = model.score(X_test[y_test==2], y_test[y_test==2])\n",
    "    return pd.Series([train_acc, test_acc, test_class0, test_class1, test_class2],\n",
    "                    index = ['Train accuracy', 'Test accuracy', \n",
    "                             \"Test accuracy CN\", \"Test accuracy CI\", \"Test accuracy AD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `stratified` strategy to generate predictions as a simple baseline to compare with other classifiers we learned in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Classifier Training Score:  0.423510466989\n",
      "Dummy Classifier Test Score:  0.444444444444\n"
     ]
    }
   ],
   "source": [
    "dc = DummyClassifier(strategy='stratified', random_state=9001)\n",
    "dc.fit(X_train,y_train)\n",
    "print('Dummy Classifier Training Score: ', dc.score(X_train,y_train))\n",
    "print('Dummy Classifier Test Score: ', dc.score(X_test,y_test))\n",
    "dc_score = score(dc, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested 6 kinds of logistic regression, logistic regression with l1 penalty, logistic regression with l2 penalty, unweighted logistic regression, weighted logistic regression, one-vs-rest logistic regression and multinomial logistic regression. We chose the best parameters with cross validation. We found that unless we used weighted logistic regression, we need a large regularization term. However, the accuracy of weighted logistic regression is very low compared to the others. That indicates that we have too many variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization strength: \n",
      "-------------------------\n",
      "Logistic regression with l1 penalty: 2.78255940221\n",
      "Logistic regression with l2 penalty: 0.35938136638\n",
      "Unweighted logistic regression:  0.35938136638\n",
      "Weighted logistic regression:  1291.54966501\n",
      "OVR logistic regression:  0.35938136638\n",
      "Multinomial logistic regression:  21.5443469003\n"
     ]
    }
   ],
   "source": [
    "#l1\n",
    "log_l1 = LogisticRegressionCV(penalty = 'l1', solver = 'liblinear', random_state=9001)\n",
    "log_l1.fit(X_train,y_train)\n",
    "\n",
    "#l2\n",
    "log_l2 = LogisticRegressionCV(penalty = 'l2', random_state=9001)\n",
    "log_l2.fit(X_train,y_train)\n",
    "\n",
    "#Unweighted logistic regression\n",
    "unweighted_logistic = LogisticRegressionCV(random_state=9001)\n",
    "unweighted_logistic.fit(X_train,y_train)\n",
    "\n",
    "#Weighted logistic regression\n",
    "weighted_logistic = LogisticRegressionCV(class_weight='balanced', random_state=9001)\n",
    "weighted_logistic.fit(X_train,y_train)\n",
    "\n",
    "#ovr\n",
    "log_ovr = LogisticRegressionCV(multi_class = 'ovr', random_state=9001)\n",
    "log_ovr.fit(X_train,y_train)\n",
    "\n",
    "#multinomial\n",
    "log_multinomial = LogisticRegressionCV(multi_class = 'multinomial', solver = 'newton-cg', random_state=9001)\n",
    "log_multinomial.fit(X_train,y_train)\n",
    "\n",
    "print(\"Regularization strength: \")\n",
    "print(\"-------------------------\")\n",
    "print(\"Logistic regression with l1 penalty:\", log_l1.C_[0])\n",
    "print(\"Logistic regression with l2 penalty:\", log_l2.C_[0])\n",
    "print(\"Unweighted logistic regression: \", unweighted_logistic.C_[0])\n",
    "print(\"Weighted logistic regression: \", weighted_logistic.C_[0])\n",
    "print(\"OVR logistic regression: \", log_ovr.C_[0])\n",
    "print(\"Multinomial logistic regression: \", log_multinomial.C_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy\n",
      "-------------------------------------------------\n",
      "Logistic Regression with l1 penalty train Score:  0.82769726248\n",
      "Logistic Regression with l2 penalty train Score:  0.618357487923\n",
      "Unweighted Logistic Regression with train Score:  0.618357487923\n",
      "Weighted Logistic Regression train Score:  0.484702093398\n",
      "OVR Logistic Regression train Score:  0.618357487923\n",
      "Multinomial Logistic Regression train Score:  0.840579710145\n",
      "\n",
      "\n",
      "Test accuracy\n",
      "-------------------------------------------------\n",
      "Logistic Regression with l1 penalty test Score:  0.783950617284\n",
      "Logistic Regression with l2 penalty test Score:  0.592592592593\n",
      "Unweighted Logistic Regression test Score:  0.592592592593\n",
      "Weighted Logistic Regression test Score:  0.425925925926\n",
      "OVR Logistic Regression test Score:  0.592592592593\n",
      "Multinomial Logistic Regression test Score:  0.746913580247\n"
     ]
    }
   ],
   "source": [
    "#Computing the score on the train set - \n",
    "print(\"Training accuracy\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print('Logistic Regression with l1 penalty train Score: ',log_l1.score(X_train, y_train))\n",
    "print('Logistic Regression with l2 penalty train Score: ',log_l2.score(X_train, y_train))\n",
    "print('Unweighted Logistic Regression with train Score: ',unweighted_logistic.score(X_train, y_train))\n",
    "print('Weighted Logistic Regression train Score: ',weighted_logistic.score(X_train, y_train))\n",
    "print('OVR Logistic Regression train Score: ',log_ovr.score(X_train, y_train))\n",
    "print('Multinomial Logistic Regression train Score: ',log_multinomial.score(X_train, y_train))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#Computing the score on the test set - \n",
    "print(\"Test accuracy\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print('Logistic Regression with l1 penalty test Score: ',log_l1.score(X_test, y_test))\n",
    "print('Logistic Regression with l2 penalty test Score: ',log_l2.score(X_test, y_test))\n",
    "print('Unweighted Logistic Regression test Score: ',unweighted_logistic.score(X_test, y_test))\n",
    "print('Weighted Logistic Regression test Score: ',weighted_logistic.score(X_test, y_test))\n",
    "print('OVR Logistic Regression test Score: ',log_ovr.score(X_test, y_test))\n",
    "print('Multinomial Logistic Regression test Score: ',log_multinomial.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store the accuracy score\n",
    "l1_score = score(log_l1, X_train, y_train, X_test, y_test)\n",
    "l2_score = score(log_l2, X_train, y_train, X_test, y_test)\n",
    "weighted_score = score(weighted_logistic, X_train, y_train, X_test, y_test)\n",
    "unweighted_score = score(unweighted_logistic, X_train, y_train, X_test, y_test)\n",
    "ovr_score = score(log_ovr, X_train, y_train, X_test, y_test)\n",
    "multi_score = score(log_multinomial, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "Logistic Regression with l1 penalty:\n",
      " [[24 18  0]\n",
      " [10 80  3]\n",
      " [ 0  4 23]]\n",
      "Logistic Regression with l2 penalty:\n",
      " [[ 0 42  0]\n",
      " [ 0 86  7]\n",
      " [ 0 17 10]]\n",
      "Unweighted Logistic Regression:\n",
      " [[ 0 42  0]\n",
      " [ 0 86  7]\n",
      " [ 0 17 10]]\n",
      "Weighted Logistic Regression:\n",
      " [[35  3  4]\n",
      " [50 16 27]\n",
      " [ 3  6 18]]\n",
      "OVR Logistic Regression:\n",
      " [[ 0 42  0]\n",
      " [ 0 86  7]\n",
      " [ 0 17 10]]\n",
      "Multinomial Logistic Regression:\n",
      " [[27 15  0]\n",
      " [17 71  5]\n",
      " [ 0  4 23]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "l1_pred = log_l1.predict(X_test)\n",
    "l2_pred = log_l2.predict(X_test)\n",
    "weighted_pred = weighted_logistic.predict(X_test)\n",
    "unweighted_pred = unweighted_logistic.predict(X_test)\n",
    "ovr_pred = log_ovr.predict(X_test)\n",
    "multi_pred = log_multinomial.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"Logistic Regression with l1 penalty:\\n\",\n",
    "      confusion_matrix(y_test, l1_pred))\n",
    "print(\"Logistic Regression with l2 penalty:\\n\",\n",
    "      confusion_matrix(y_test, l2_pred))\n",
    "print(\"Unweighted Logistic Regression:\\n\",\n",
    "      confusion_matrix(y_test, unweighted_pred))\n",
    "print(\"Weighted Logistic Regression:\\n\",\n",
    "      confusion_matrix(y_test, weighted_pred))\n",
    "print(\"OVR Logistic Regression:\\n\",\n",
    "      confusion_matrix(y_test, ovr_pred))\n",
    "print(\"Multinomial Logistic Regression:\\n\",\n",
    "      confusion_matrix(y_test, multi_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed normalization on continuous predictors and used Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) as our models. LDA performs really well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalization\n",
    "cols_standardize = [\n",
    "    c for c in X_train.columns \n",
    "    if (not c.startswith('PT')) or (c=='PTEDUCAT')]\n",
    "\n",
    "X_train_std = X_train.copy()\n",
    "X_test_std = X_test.copy()\n",
    "for c in cols_standardize:\n",
    "    col_mean = np.mean(X_train[c])\n",
    "    col_sd = np.std(X_train[c])\n",
    "    if col_sd > (1e-10)*col_mean:\n",
    "        X_train_std[c] = (X_train[c]-col_mean)/col_sd\n",
    "        X_test_std[c] = (X_test[c]-col_mean)/col_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>PTRACCAT_Asian</th>\n",
       "      <th>PTRACCAT_Black</th>\n",
       "      <th>PTRACCAT_Hawaiian/Other_PI</th>\n",
       "      <th>PTRACCAT_More_than_one</th>\n",
       "      <th>PTRACCAT_Unknown</th>\n",
       "      <th>PTRACCAT_White</th>\n",
       "      <th>PTETHCAT_Not_Hisp/Latino</th>\n",
       "      <th>PTMARRY_Married</th>\n",
       "      <th>...</th>\n",
       "      <th>WholeBrain</th>\n",
       "      <th>WholeBrain_slope</th>\n",
       "      <th>Entorhinal</th>\n",
       "      <th>Entorhinal_slope</th>\n",
       "      <th>Fusiform</th>\n",
       "      <th>Fusiform_slope</th>\n",
       "      <th>MidTemp</th>\n",
       "      <th>MidTemp_slope</th>\n",
       "      <th>ICV</th>\n",
       "      <th>ICV_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.852257</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.761500</td>\n",
       "      <td>-0.567555</td>\n",
       "      <td>-0.820814</td>\n",
       "      <td>-1.269796</td>\n",
       "      <td>-1.426968</td>\n",
       "      <td>0.156847</td>\n",
       "      <td>-2.102069</td>\n",
       "      <td>-0.192827</td>\n",
       "      <td>-1.574482</td>\n",
       "      <td>0.093937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.376909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134464</td>\n",
       "      <td>-0.028641</td>\n",
       "      <td>-0.070387</td>\n",
       "      <td>0.188014</td>\n",
       "      <td>0.721399</td>\n",
       "      <td>-0.067438</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.506511</td>\n",
       "      <td>-0.489132</td>\n",
       "      <td>-0.265646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.607970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.300396</td>\n",
       "      <td>0.310720</td>\n",
       "      <td>0.456478</td>\n",
       "      <td>-0.560840</td>\n",
       "      <td>0.292776</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>-0.650452</td>\n",
       "      <td>0.224140</td>\n",
       "      <td>-1.239633</td>\n",
       "      <td>-0.014198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.160970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.003749</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>-0.003683</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.015345</td>\n",
       "      <td>0.018697</td>\n",
       "      <td>0.004091</td>\n",
       "      <td>-0.005136</td>\n",
       "      <td>0.004314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.160970</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.003749</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>-0.003683</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.015345</td>\n",
       "      <td>0.018697</td>\n",
       "      <td>0.004091</td>\n",
       "      <td>1.652198</td>\n",
       "      <td>-0.047345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PTGENDER  PTEDUCAT  PTRACCAT_Asian  PTRACCAT_Black  \\\n",
       "0         0 -2.852257               0               0   \n",
       "1         1  1.376909               0               0   \n",
       "2         0  0.607970               0               0   \n",
       "3         0 -0.160970               0               0   \n",
       "4         1 -0.160970               0               0   \n",
       "\n",
       "   PTRACCAT_Hawaiian/Other_PI  PTRACCAT_More_than_one  PTRACCAT_Unknown  \\\n",
       "0                           0                       0                 0   \n",
       "1                           0                       0                 0   \n",
       "2                           0                       0                 0   \n",
       "3                           0                       0                 0   \n",
       "4                           0                       0                 0   \n",
       "\n",
       "   PTRACCAT_White  PTETHCAT_Not_Hisp/Latino  PTMARRY_Married    ...      \\\n",
       "0               1                         1                0    ...       \n",
       "1               1                         1                1    ...       \n",
       "2               1                         1                1    ...       \n",
       "3               1                         1                0    ...       \n",
       "4               1                         1                0    ...       \n",
       "\n",
       "   WholeBrain  WholeBrain_slope  Entorhinal  Entorhinal_slope  Fusiform  \\\n",
       "0   -1.761500         -0.567555   -0.820814         -1.269796 -1.426968   \n",
       "1   -0.134464         -0.028641   -0.070387          0.188014  0.721399   \n",
       "2   -1.300396          0.310720    0.456478         -0.560840  0.292776   \n",
       "3   -0.000094         -0.003749    0.006635         -0.003683  0.010325   \n",
       "4   -0.000094         -0.003749    0.006635         -0.003683  0.010325   \n",
       "\n",
       "   Fusiform_slope   MidTemp  MidTemp_slope       ICV  ICV_slope  \n",
       "0        0.156847 -2.102069      -0.192827 -1.574482   0.093937  \n",
       "1       -0.067438  0.019784       0.506511 -0.489132  -0.265646  \n",
       "2        0.016824 -0.650452       0.224140 -1.239633  -0.014198  \n",
       "3        0.015345  0.018697       0.004091 -0.005136   0.004314  \n",
       "4        0.015345  0.018697       0.004091  1.652198  -0.047345  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy\n",
      "------------------\n",
      "LDA Train Score:  0.85346215781\n",
      "QDA Train Score:  0.816425120773\n",
      "\n",
      "\n",
      "Test accuracy\n",
      "------------------\n",
      "LDA Test Score:  0.796296296296\n",
      "QDA Test Score:  0.716049382716\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "lda.fit(X_train_std,y_train)\n",
    "qda.fit(X_train_std,y_train)\n",
    "\n",
    "# training accuracy\n",
    "print(\"Training accuracy\")\n",
    "print(\"------------------\")\n",
    "print('LDA Train Score: ',lda.score(X_train_std,y_train))\n",
    "print('QDA Train Score: ',qda.score(X_train_std,y_train))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# test accuracy\n",
    "print(\"Test accuracy\")\n",
    "print(\"------------------\")\n",
    "print('LDA Test Score: ',lda.score(X_test_std,y_test))\n",
    "print('QDA Test Score: ',qda.score(X_test_std,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store the accuracy score\n",
    "lda_score = score(lda, X_train_std, y_train, X_test_std, y_test)\n",
    "qda_score = score(qda, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "LDA:\n",
      " [[26 16  0]\n",
      " [ 9 79  5]\n",
      " [ 1  2 24]]\n",
      "QDA:\n",
      " [[29 13  0]\n",
      " [12 66 15]\n",
      " [ 0  6 21]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "lda_pred = lda.predict(X_test_std)\n",
    "qda_pred = qda.predict(X_test_std)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"LDA:\\n\",\n",
    "      confusion_matrix(y_test, lda_pred))\n",
    "print(\"QDA:\\n\",\n",
    "      confusion_matrix(y_test, qda_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of neighbours is 37, which is a relatively large number considering that we only have 783 observations. The accuracy is not satisfactory as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of neighbours:  41\n",
      "KNN Training Accuracy:  0.566827697262\n",
      "KNN Test Accuracy:  0.574074074074\n"
     ]
    }
   ],
   "source": [
    "cv_fold = KFold(n_splits=5, shuffle=True, random_state=9001)\n",
    "\n",
    "max_score = 0\n",
    "max_k = 0 \n",
    "\n",
    "for k in range(1,60):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn_val_score = cross_val_score(knn, X_train, y_train, cv=cv_fold).mean()\n",
    "    if knn_val_score > max_score:\n",
    "        max_k = k\n",
    "        max_score = knn_val_score\n",
    "        \n",
    "knn = KNeighborsClassifier(n_neighbors = max_k)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "print(\"Optimal number of neighbours: \", max_k)\n",
    "print('KNN Training Accuracy: ', knn.score(X_train,y_train))\n",
    "print('KNN Test Accuracy: ', knn.score(X_test,y_test))\n",
    "\n",
    "# Store the accuracy score\n",
    "knn_score = score(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Confusion Matrix:\n",
      " [[ 0 42  0]\n",
      " [ 0 92  1]\n",
      " [ 1 25  1]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "knn_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"KNN Confusion Matrix:\\n\",\n",
    "      confusion_matrix(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used 5-fold cross validation to find the optimal depth for the decision tree. The optimal depth is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal depth: 6\n"
     ]
    }
   ],
   "source": [
    "depth = []\n",
    "for i in range(3,20):\n",
    "    dt = DecisionTreeClassifier(max_depth=i, random_state=9001)\n",
    "    # Perform 5-fold cross validation \n",
    "    scores = cross_val_score(estimator=dt, X=X_train, y=y_train, cv=cv_fold, n_jobs=-1)\n",
    "    depth.append((i, scores.mean(), scores.std())) \n",
    "depthvals = [t[0] for t in depth]\n",
    "cvmeans = np.array([t[1] for t in depth])\n",
    "cvstds = np.array([t[2] for t in depth])\n",
    "max_indx = np.argmax(cvmeans)\n",
    "md_best = depthvals[max_indx]\n",
    "print('Optimal depth:',md_best)\n",
    "dt_best = DecisionTreeClassifier(max_depth=md_best, random_state=9001)\n",
    "dt_best.fit(X_train, y_train).score(X_test, y_test)\n",
    "dt_score = score(dt_best, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Training Accuracy:  0.90499194847\n",
      "Decision Tree Test Accuracy:  0.734567901235\n"
     ]
    }
   ],
   "source": [
    "print('Decision Tree Training Accuracy: ', dt_best.score(X_train,y_train))\n",
    "print('Decision Tree Test Accuracy: ', dt_best.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Confusion Matrix:\n",
      " [[24 18  0]\n",
      " [21 71  1]\n",
      " [ 0  3 24]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "dt_pred = dt_best.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Confusion Matrix:\\n\",\n",
    "      confusion_matrix(y_test, dt_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `GridSearchCV` to find the optimal number of trees and tree depth. We then used the optimal value to perform random forest classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of trees 16, tree depth: 8\n",
      "\n",
      "\n",
      "Random Forest Training Accuracy:  0.972624798712\n",
      "Random Forest Test Accuracy:  0.796296296296\n"
     ]
    }
   ],
   "source": [
    "trees = [2**x for x in range(8)]  # 1, 2, 4, 8, 16, 32, ...\n",
    "depth = [2, 4, 6, 8, 10]\n",
    "parameters = {'n_estimators': trees,\n",
    "              'max_depth': depth}\n",
    "rf = RandomForestClassifier(random_state=9001)\n",
    "rf_cv = GridSearchCV(rf, parameters, cv=cv_fold)\n",
    "rf_cv.fit(X_train, y_train)\n",
    "best_score = np.argmax(rf_cv.cv_results_['mean_test_score'])\n",
    "result = rf_cv.cv_results_['params'][best_score]\n",
    "opt_depth = result['max_depth']\n",
    "opt_tree = result['n_estimators']\n",
    "print(\"Optimal number of trees {}, tree depth: {}\".format(opt_tree, opt_depth))\n",
    "rf = RandomForestClassifier(n_estimators=opt_tree, max_depth=opt_depth, random_state=9001)\n",
    "rf.fit(X_train, y_train)\n",
    "print('\\n')\n",
    "print('Random Forest Training Accuracy: ', rf.score(X_train,y_train))\n",
    "print('Random Forest Test Accuracy: ', rf.score(X_test,y_test))\n",
    "rf_score = score(rf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Confusion Matrix:\n",
      " [[20 22  0]\n",
      " [ 4 87  2]\n",
      " [ 0  5 22]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Confusion Matrix:\\n\",\n",
    "      confusion_matrix(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the optimal tree depth found by cross validation in the decision tree classifier, and performed `GridSearchCV` to find the optimal number of trees and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of trees 16, learning rate: 1\n",
      "\n",
      "\n",
      "AdaBoost Training Accuracy:  1.0\n",
      "AdaBoost Test Accuracy:  0.777777777778\n"
     ]
    }
   ],
   "source": [
    "trees = [2**x for x in range(6)]  # 1, 2, 4, 8, 16, 32, ...\n",
    "learning_rate = [0.1, 0.5, 1, 5]\n",
    "parameters = {'n_estimators': trees,\n",
    "              'learning_rate': learning_rate}\n",
    "ab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=md_best),\n",
    "                        random_state=9001)\n",
    "ab_cv = GridSearchCV(ab, parameters, cv=cv_fold)\n",
    "ab_cv.fit(X_train, y_train)\n",
    "best_score = np.argmax(ab_cv.cv_results_['mean_test_score'])\n",
    "result = ab_cv.cv_results_['params'][best_score]\n",
    "opt_learning_rate = result['learning_rate']\n",
    "opt_tree = result['n_estimators']\n",
    "print(\"Optimal number of trees {}, learning rate: {}\".format(opt_tree, opt_learning_rate))\n",
    "ab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=md_best), n_estimators=opt_tree,\n",
    "                       learning_rate=opt_learning_rate, random_state=9001)\n",
    "ab.fit(X_train, y_train)\n",
    "print('\\n')\n",
    "print('AdaBoost Training Accuracy: ', ab.score(X_train,y_train))\n",
    "print('AdaBoost Test Accuracy: ', ab.score(X_test,y_test))\n",
    "ab_score = score(ab, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Confusion Matrix:\n",
      " [[22 20  0]\n",
      " [ 7 80  6]\n",
      " [ 0  3 24]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "ab_pred = ab.predict(X_test)\n",
    "\n",
    "print(\"AdaBoost Confusion Matrix:\\n\",\n",
    "      confusion_matrix(y_test, ab_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Dummy Classifier</th>\n",
       "      <th>KNN</th>\n",
       "      <th>LDA</th>\n",
       "      <th>Logistic Regression with l1</th>\n",
       "      <th>Logistic Regression with l2</th>\n",
       "      <th>Multinomial</th>\n",
       "      <th>OVR</th>\n",
       "      <th>QDA</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Unweighted logistic</th>\n",
       "      <th>Weighted logistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904992</td>\n",
       "      <td>0.423510</td>\n",
       "      <td>0.566828</td>\n",
       "      <td>0.853462</td>\n",
       "      <td>0.827697</td>\n",
       "      <td>0.618357</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.618357</td>\n",
       "      <td>0.816425</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.618357</td>\n",
       "      <td>0.484702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test accuracy</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.734568</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.783951</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.746914</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.425926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test accuracy CN</th>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test accuracy CI</th>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.989247</td>\n",
       "      <td>0.849462</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.172043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test accuracy AD</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AdaBoost  Decision Tree  Dummy Classifier       KNN  \\\n",
       "Train accuracy    1.000000       0.904992          0.423510  0.566828   \n",
       "Test accuracy     0.777778       0.734568          0.444444  0.574074   \n",
       "Test accuracy CN  0.523810       0.571429          0.333333  0.000000   \n",
       "Test accuracy CI  0.860215       0.763441          0.516129  0.989247   \n",
       "Test accuracy AD  0.888889       0.888889          0.222222  0.037037   \n",
       "\n",
       "                       LDA  Logistic Regression with l1  \\\n",
       "Train accuracy    0.853462                     0.827697   \n",
       "Test accuracy     0.796296                     0.783951   \n",
       "Test accuracy CN  0.619048                     0.571429   \n",
       "Test accuracy CI  0.849462                     0.860215   \n",
       "Test accuracy AD  0.888889                     0.851852   \n",
       "\n",
       "                  Logistic Regression with l2  Multinomial       OVR  \\\n",
       "Train accuracy                       0.618357     0.840580  0.618357   \n",
       "Test accuracy                        0.592593     0.746914  0.592593   \n",
       "Test accuracy CN                     0.000000     0.642857  0.000000   \n",
       "Test accuracy CI                     0.924731     0.763441  0.924731   \n",
       "Test accuracy AD                     0.370370     0.851852  0.370370   \n",
       "\n",
       "                       QDA  Random Forest  Unweighted logistic  \\\n",
       "Train accuracy    0.816425       0.972625             0.618357   \n",
       "Test accuracy     0.716049       0.796296             0.592593   \n",
       "Test accuracy CN  0.690476       0.476190             0.000000   \n",
       "Test accuracy CI  0.709677       0.935484             0.924731   \n",
       "Test accuracy AD  0.777778       0.814815             0.370370   \n",
       "\n",
       "                  Weighted logistic  \n",
       "Train accuracy             0.484702  \n",
       "Test accuracy              0.425926  \n",
       "Test accuracy CN           0.833333  \n",
       "Test accuracy CI           0.172043  \n",
       "Test accuracy AD           0.666667  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame({'Dummy Classifier': dc_score,\n",
    "                         'Logistic Regression with l1': l1_score, \n",
    "                         'Logistic Regression with l2': l2_score,\n",
    "                         'Weighted logistic': weighted_score,\n",
    "                         'Unweighted logistic': unweighted_score,\n",
    "                         'OVR': ovr_score,\n",
    "                         'Multinomial': multi_score,\n",
    "                         'KNN': knn_score,\n",
    "                         'LDA': lda_score,\n",
    "                         'QDA': qda_score,\n",
    "                         'Decision Tree': dt_score,\n",
    "                         'Random Forest': rf_score,\n",
    "                         'AdaBoost': ab_score})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For baseline models, people usually use the dummy classifier with \"stratified\" strategy or the \"most frequent\" strategy. The \"stratified\" method generates prediction according to the class distribution of the training set. The \"most frequent\" strategy always predict the most frequent class. We adopted the `stratified` strategy implemented by `sklearn`'s `DummyClassifer` as the baseline model. As can be seen above, all other classification models we used outperformed the dummy classifier as expected.\n",
    "\n",
    "Based on the above summary, AdaBoost has a very high training accuracy 100%. Random forest classifier has the second highest training accuracy which is close to 1(0.972625), and it also has the highest accuracy(0.796296) on the test set. LDA has the same test accuracy(0.796296) as random forest classifier, and logistic regression with l1 regularization is the third highest(0.783951).\n",
    "\n",
    "For classifying `CN` patients, weighted logistic regression has the highest test accuracy(0.833333), so it performed the best for determining Cognitively Normal patients. However, KNN, logistic regression with l2 regularization, OvR logistic regression and unweighted logistic regression have zero accuracy on classifying `CN` patients. Since all of them have very high accuracy on `CI` but low accuracy on `AD`, we think these four models probably classified almost all the `CN` patients into `CI` (as can been seen in the confusion matrix) which leads to zero accuracy on `CN` and high accuracy on `CI`.\n",
    "\n",
    "KNN has the highest test accuracy(0.989247) on diagnosing `CI` cognitive impairment patients. Logistic regression with l2 regularization, random forest classifier, OvR logistic regression and unweighted logistic regression all reached 0.9 accuracy on diagnosing `CI` patients.\n",
    "\n",
    "Since we focus on the diagnosis of Alzheimer's disease, we are more concerned about the test accuracy on `AD` patients. AdaBoost, LDA and decision tree classifier have the highest test accuracy(0.888889) on `AD` patients. Logistic regression with l1 regularization, random forest classifier and multinomial logistic regression all reached test accuracy of over 0.8 on the classification of `AD`.\n",
    "\n",
    "To conclude, random forest classifier and LDA performed the best if we are concerned about both overall test accuracy and correctly diagnosing `AD` patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
